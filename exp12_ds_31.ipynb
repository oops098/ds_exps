{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Supervised Machine Learning refers to the task of learning a function that maps input data\n",
        "(features) to output labels (target) based on labeled training data. The learning process creates\n",
        "a model that can predict the label for unseen data. This type of learning is divided into\n",
        "classification (predicting categorical labels) and regression (predicting continuous values).\n",
        "1. Naive Bayes (NB)\n",
        "Naive Bayes is a simple, probabilistic classifier based on Bayes’ Theorem with the assumption\n",
        "that features are conditionally independent.\n",
        "How it works:\n",
        "● Bayes' theorem calculates the probability of each class given the input data and assigns\n",
        "the class with the highest probability.\n",
        "● Assumes that the presence of a feature is independent of other features.\n"
      ],
      "metadata": {
        "id": "6Qg1BHsn75LM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8ir6h-Z70p7",
        "outputId": "f80a264b-b595-410f-ad44-5008057dd4ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Accuracy: 0.9778\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Naive Bayes Classifier\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "y_pred = nb.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Naive Bayes Accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Random Forest (RF)\n",
        "Random Forest is an ensemble learning method that constructs a collection of decision trees,\n",
        "each trained on random subsets of the data, and outputs the majority vote of these trees.\n",
        "How it works:\n",
        "● Each decision tree is trained on a bootstrap sample (random subset) of the data.\n",
        "● The final prediction is based on the majority vote (for classification) or average (for\n",
        "regression) of the individual trees.\n"
      ],
      "metadata": {
        "id": "r48CAGCF8Quc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy = rf.score(X_test, y_test)\n",
        "print(f\"Random Forest Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qml46GNa8Vy4",
        "outputId": "b7847c73-c9a8-4ad1-dc77-02b0e7895a2e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Tree classifiers use a tree-like structure where each internal node represents a\n",
        "feature split, each branch represents an outcome, and each leaf represents a class label.\n",
        "How it works:\n",
        "● Splits data into subsets using a criterion (like Gini impurity or information gain).\n",
        "● Repeats this process until a stopping condition is met (e.g., maximum depth, all leaves\n",
        "pure)."
      ],
      "metadata": {
        "id": "Eme7QrG78hGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "test_size=0.3)\n",
        "# Train Decision Tree Classifier\n",
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(X_train, y_train)\n",
        "# Evaluate model\n",
        "accuracy = dt.score(X_test, y_test)\n",
        "print(f\"Decision Tree Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r66BX3_u8k2J",
        "outputId": "0a01b3c2-4c97-4dd7-cfd2-197ffd264a1f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 0.9333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Machine is a classifier that finds the optimal hyperplane (in multi-dimensional\n",
        "space) that maximally separates classes.\n",
        "How it works:\n",
        "\n",
        "● SVM finds a hyperplane that maximizes the margin between the classes.\n",
        "\n",
        "● For non-linearly separable data, it uses kernel functions to project the data into a\n",
        "higher-dimensional space.\n"
      ],
      "metadata": {
        "id": "dsQSrhp38p4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM Classifier\n",
        "svm = SVC(kernel='linear', random_state=42)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy = svm.score(X_test, y_test)\n",
        "print(f\"SVM Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-lAvrrL8vb6",
        "outputId": "e9c3eb13-5ff9-434c-8d87-9d18b2156d9e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Nearest Neighbors is an instance-based algorithm that assigns a class to a new data point\n",
        "based on the majority class of its k nearest neighbors.\n",
        "How it works:\n",
        "\n",
        "● The algorithm calculates the distance (e.g., Euclidean) between the test point and all training data points.\n",
        "\n",
        "● The majority class of the k nearest points is assigned as the class label."
      ],
      "metadata": {
        "id": "OFyzI7Js88p8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "test_size=0.3)\n",
        "# Train KNN Classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "# Evaluate model\n",
        "accuracy = knn.score(X_test, y_test)\n",
        "print(f\"KNN Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avjvvVui9DJC",
        "outputId": "2db28d8b-958c-4df4-d5a9-5f124822cf1d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Accuracy: 0.9111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost is an optimized implementation of Gradient Boosting, a powerful ensemble technique\n",
        "that sequentially builds models to correct errors from the previous ones.\n",
        "How it works:\n",
        "\n",
        "● XGBoost builds trees sequentially, where each new tree tries to correct the errors of the\n",
        "previous ones.\n",
        "\n",
        "● It uses gradient descent to minimize the loss function and incorporates regularization to\n",
        "prevent overfitting.\n"
      ],
      "metadata": {
        "id": "j633q9CW9IcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train XGBoost Classifier\n",
        "xgb = XGBClassifier(eval_metric='mlogloss', random_state=42)\n",
        "xgb.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy = xgb.score(X_test, y_test)\n",
        "print(f\"XGBoost Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwSItMLw9N-7",
        "outputId": "e8365a8a-ba77-4f41-96b5-6f0c5e72a112"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AdaBoost is an ensemble method that combines multiple weak learners (typically decision\n",
        "stumps) into a strong classifier. It adjusts the weight of misclassified points, making subsequent\n",
        "classifiers focus on the hard-to-classify points.\n",
        "How it works:\n",
        "\n",
        "● Each weak learner (e.g., decision stump) is trained on weighted data, with higher\n",
        "weights given to previously misclassified points.\n",
        "\n",
        "● The final classifier is a weighted vote of all weak classifiers."
      ],
      "metadata": {
        "id": "qQYsLs739jPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train AdaBoost Classifier with Decision Tree base estimator\n",
        "ada = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1), n_estimators=50)\n",
        "ada.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy = ada.score(X_test, y_test)\n",
        "print(f\"AdaBoost Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OAf0hI39q-T",
        "outputId": "d3a11f45-3fac-4513-928d-c50577473bcf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}